{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528a5f01-059a-4947-9063-6380365a0ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped data shape: (99, 9000, 3),(99, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# 加载.mat文件\n",
    "data = loadmat('further_cleaned_data.mat')\n",
    "# 提取 AE_spindle 数据\n",
    "ae_spindle_data = data['AE_spindle']\n",
    "vib_spindle_data = data['vib_spindle']\n",
    "ae_table_data = data['AE_table']\n",
    "vib_table_data = data['vib_table']\n",
    "VB_data = data['VB']\n",
    "\n",
    "# Function to remove one in every three values\n",
    "# def reduce_values(data_array):\n",
    "#     return data_array[:, np.delete(np.arange(data_array.shape[1]), np.arange(2, data_array.shape[1], 3))]\n",
    "\n",
    "# Applying the function to each of the variables\n",
    "# AE_spindle = reduce_values(ae_spindle_data)\n",
    "# AE_table = reduce_values(ae_table_data)\n",
    "# vib_spindle = reduce_values(vib_spindle_data)\n",
    "# vib_table = reduce_values(vib_table_data)\n",
    "AE_spindle = ae_spindle_data\n",
    "AE_table = ae_table_data\n",
    "vib_spindle = vib_spindle_data\n",
    "vib_table = vib_table_data\n",
    "\n",
    "# ae_spindle_reduced = AE_spindle[:, start_index_ae:end_index_ae]\n",
    "# vib_spindle_reduced = vib_spindle[:, start_index_vb:end_index_vb]\n",
    "# ae_table_reduced = AE_table[:, start_index_ae:end_index_ae]\n",
    "# vib_table_reduced = vib_table[:, start_index_vb:end_index_vb]\n",
    "\n",
    "\n",
    "combined_feature = np.stack((AE_spindle,vib_table,vib_spindle), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Reshaped data shape: {combined_feature.shape},{VB_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff18b3-e91a-4960-bf20-0b0075937e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e9997-fcbb-4fe4-9ab8-efe98fe7ba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17331\\.conda\\envs\\MachineLearn\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\17331\\.conda\\envs\\MachineLearn\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:34: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 19s/step - loss: 0.0983 - r_squared: -3.7470 - val_loss: 0.1171 - val_r_squared: -2.3794 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 19s/step - loss: 0.0748 - r_squared: -2.3377 - val_loss: 0.0963 - val_r_squared: -1.7777 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 19s/step - loss: 0.0612 - r_squared: -1.6800 - val_loss: 0.0790 - val_r_squared: -1.2794 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 19s/step - loss: 0.0481 - r_squared: -1.2230 - val_loss: 0.0656 - val_r_squared: -0.8935 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 19s/step - loss: 0.0444 - r_squared: -1.0649 - val_loss: 0.0563 - val_r_squared: -0.6249 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 19s/step - loss: 0.0428 - r_squared: -1.0274 - val_loss: 0.0506 - val_r_squared: -0.4602 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 19s/step - loss: 0.0408 - r_squared: -0.9170 - val_loss: 0.0473 - val_r_squared: -0.3650 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 19s/step - loss: 0.0385 - r_squared: -0.9349 - val_loss: 0.0460 - val_r_squared: -0.3275 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 20s/step - loss: 0.0361 - r_squared: -0.9738 - val_loss: 0.0462 - val_r_squared: -0.3319 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 19s/step - loss: 0.0348 - r_squared: -0.6433 - val_loss: 0.0473 - val_r_squared: -0.3654 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 9.048374340636656e-05.\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 20s/step - loss: 0.0328 - r_squared: -0.7609 - val_loss: 0.0488 - val_r_squared: -0.4066 - learning_rate: 9.0484e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 8.187307685147971e-05.\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 19s/step - loss: 0.0340 - r_squared: -0.5217 - val_loss: 0.0492 - val_r_squared: -0.4188 - learning_rate: 8.1873e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 7.408182136714458e-05.\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 20s/step - loss: 0.0318 - r_squared: -0.4166 - val_loss: 0.0488 - val_r_squared: -0.4088 - learning_rate: 7.4082e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 6.70320077915676e-05.\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 20s/step - loss: 0.0311 - r_squared: -0.3860 - val_loss: 0.0480 - val_r_squared: -0.3852 - learning_rate: 6.7032e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 6.065307024982758e-05.\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 20s/step - loss: 0.0317 - r_squared: -0.3693 - val_loss: 0.0469 - val_r_squared: -0.3537 - learning_rate: 6.0653e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 5.4881169489817694e-05.\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 20s/step - loss: 0.0297 - r_squared: -0.4071 - val_loss: 0.0457 - val_r_squared: -0.3190 - learning_rate: 5.4881e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 4.965853804606013e-05.\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 20s/step - loss: 0.0300 - r_squared: -0.3233 - val_loss: 0.0443 - val_r_squared: -0.2779 - learning_rate: 4.9659e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 4.4932905439054593e-05.\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 20s/step - loss: 0.0259 - r_squared: -0.2652 - val_loss: 0.0431 - val_r_squared: -0.2444 - learning_rate: 4.4933e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 4.0656974306330085e-05.\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 20s/step - loss: 0.0248 - r_squared: -0.1978 - val_loss: 0.0421 - val_r_squared: -0.2136 - learning_rate: 4.0657e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 3.6787951103178784e-05.\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 20s/step - loss: 0.0244 - r_squared: -0.1727 - val_loss: 0.0413 - val_r_squared: -0.1908 - learning_rate: 3.6788e-05\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 3.3287116821156815e-05.\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 20s/step - loss: 0.0253 - r_squared: -0.1475 - val_loss: 0.0407 - val_r_squared: -0.1734 - learning_rate: 3.3287e-05\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 3.0119428629404865e-05.\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 20s/step - loss: 0.0243 - r_squared: -0.1080 - val_loss: 0.0402 - val_r_squared: -0.1588 - learning_rate: 3.0119e-05\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 2.7253186999587342e-05.\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 20s/step - loss: 0.0222 - r_squared: -0.1042 - val_loss: 0.0399 - val_r_squared: -0.1500 - learning_rate: 2.7253e-05\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 2.465970283083152e-05.\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 20s/step - loss: 0.0235 - r_squared: -0.1450 - val_loss: 0.0395 - val_r_squared: -0.1396 - learning_rate: 2.4660e-05\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 2.2313022782327607e-05.\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 20s/step - loss: 0.0251 - r_squared: -0.1021 - val_loss: 0.0394 - val_r_squared: -0.1356 - learning_rate: 2.2313e-05\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 2.0189658243907616e-05.\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 21s/step - loss: 0.0226 - r_squared: -0.0732 - val_loss: 0.0393 - val_r_squared: -0.1336 - learning_rate: 2.0190e-05\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 1.826835796236992e-05.\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21s/step - loss: 0.0231 - r_squared: -0.0363 - val_loss: 0.0393 - val_r_squared: -0.1325 - learning_rate: 1.8268e-05\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 1.6529893400729634e-05.\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 21s/step - loss: 0.0221 - r_squared: -0.0325 - val_loss: 0.0392 - val_r_squared: -0.1301 - learning_rate: 1.6530e-05\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 1.4956865925341845e-05.\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21s/step - loss: 0.0204 - r_squared: -0.0991 - val_loss: 0.0391 - val_r_squared: -0.1275 - learning_rate: 1.4957e-05\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 1.3533532182918862e-05.\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 21s/step - loss: 0.0217 - r_squared: -0.0648 - val_loss: 0.0389 - val_r_squared: -0.1215 - learning_rate: 1.3534e-05\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 1.2245646757946815e-05.\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21s/step - loss: 0.0209 - r_squared: 0.0184 - val_loss: 0.0387 - val_r_squared: -0.1151 - learning_rate: 1.2246e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 1.1080319382017478e-05.\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21s/step - loss: 0.0202 - r_squared: 0.0011 - val_loss: 0.0384 - val_r_squared: -0.1091 - learning_rate: 1.1080e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1.0025887604570016e-05.\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 21s/step - loss: 0.0209 - r_squared: -0.0139 - val_loss: 0.0383 - val_r_squared: -0.1044 - learning_rate: 1.0026e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 9.071798558579758e-06.\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21s/step - loss: 0.0216 - r_squared: 0.0226 - val_loss: 0.0382 - val_r_squared: -0.1011 - learning_rate: 9.0718e-06\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 8.208502549678087e-06.\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 22s/step - loss: 0.0223 - r_squared: 0.0061 - val_loss: 0.0381 - val_r_squared: -0.0981 - learning_rate: 8.2085e-06\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 7.4273602876928635e-06.\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 21s/step - loss: 0.0216 - r_squared: -0.0155 - val_loss: 0.0380 - val_r_squared: -0.0960 - learning_rate: 7.4274e-06\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 6.7205537561676465e-06.\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 21s/step - loss: 0.0202 - r_squared: 0.0150 - val_loss: 0.0379 - val_r_squared: -0.0947 - learning_rate: 6.7206e-06\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 6.0810084505646955e-06.\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 21s/step - loss: 0.0216 - r_squared: 0.0269 - val_loss: 0.0379 - val_r_squared: -0.0931 - learning_rate: 6.0810e-06\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 5.5023242566676345e-06.\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 21s/step - loss: 0.0207 - r_squared: 0.0278 - val_loss: 0.0378 - val_r_squared: -0.0913 - learning_rate: 5.5023e-06\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 4.9787090574682225e-06.\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22s/step - loss: 0.0199 - r_squared: -0.0088 - val_loss: 0.0378 - val_r_squared: -0.0896 - learning_rate: 4.9787e-06\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 4.504922344494844e-06.\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 25s/step - loss: 0.0187 - r_squared: 0.0600 - val_loss: 0.0377 - val_r_squared: -0.0890 - learning_rate: 4.5049e-06\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 4.0762224671198055e-06.\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 23s/step - loss: 0.0210 - r_squared: 0.0185 - val_loss: 0.0377 - val_r_squared: -0.0880 - learning_rate: 4.0762e-06\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 3.688318656713818e-06.\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 21s/step - loss: 0.0195 - r_squared: 0.0403 - val_loss: 0.0377 - val_r_squared: -0.0868 - learning_rate: 3.6883e-06\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 3.337328735142364e-06.\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 21s/step - loss: 0.0196 - r_squared: -0.0260 - val_loss: 0.0376 - val_r_squared: -0.0858 - learning_rate: 3.3373e-06\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 3.019740006493521e-06.\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22s/step - loss: 0.0201 - r_squared: -0.0085 - val_loss: 0.0376 - val_r_squared: -0.0839 - learning_rate: 3.0197e-06\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 2.732373786784592e-06.\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 22s/step - loss: 0.0210 - r_squared: 0.0491 - val_loss: 0.0375 - val_r_squared: -0.0828 - learning_rate: 2.7324e-06\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 2.472354026394896e-06.\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 22s/step - loss: 0.0194 - r_squared: 0.0358 - val_loss: 0.0375 - val_r_squared: -0.0820 - learning_rate: 2.4724e-06\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 2.2370784336089855e-06.\n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 22s/step - loss: 0.0206 - r_squared: 0.0861 - val_loss: 0.0375 - val_r_squared: -0.0812 - learning_rate: 2.2371e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 2.02419232664397e-06.\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 22s/step - loss: 0.0195 - r_squared: 0.0537 - val_loss: 0.0375 - val_r_squared: -0.0806 - learning_rate: 2.0242e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 1.8315649867872708e-06.\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 22s/step - loss: 0.0198 - r_squared: 0.0565 - val_loss: 0.0374 - val_r_squared: -0.0800 - learning_rate: 1.8316e-06\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 1.657268512644805e-06.\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 22s/step - loss: 0.0193 - r_squared: -0.0098 - val_loss: 0.0374 - val_r_squared: -0.0791 - learning_rate: 1.6573e-06\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 1.4995586070654099e-06.\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 22s/step - loss: 0.0205 - r_squared: 0.0739 - val_loss: 0.0374 - val_r_squared: -0.0783 - learning_rate: 1.4996e-06\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 1.3568567283073207e-06.\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 22s/step - loss: 0.0184 - r_squared: 0.0657 - val_loss: 0.0373 - val_r_squared: -0.0775 - learning_rate: 1.3569e-06\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 1.2277347423150786e-06.\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 22s/step - loss: 0.0202 - r_squared: 0.0710 - val_loss: 0.0373 - val_r_squared: -0.0768 - learning_rate: 1.2277e-06\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 1.110900370804302e-06.\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23s/step - loss: 0.0209 - r_squared: 0.0599 - val_loss: 0.0373 - val_r_squared: -0.0762 - learning_rate: 1.1109e-06\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 1.0051842309621861e-06.\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23s/step - loss: 0.0189 - r_squared: -0.0237 - val_loss: 0.0373 - val_r_squared: -0.0760 - learning_rate: 1.0052e-06\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 9.09528296233475e-07.\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 23s/step - loss: 0.0192 - r_squared: 0.0905 - val_loss: 0.0373 - val_r_squared: -0.0757 - learning_rate: 9.0953e-07\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 8.229752666011336e-07.\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 22s/step - loss: 0.0185 - r_squared: 0.0260 - val_loss: 0.0373 - val_r_squared: -0.0755 - learning_rate: 8.2298e-07\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 7.446588483617234e-07.\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 22s/step - loss: 0.0202 - r_squared: 0.0966 - val_loss: 0.0373 - val_r_squared: -0.0753 - learning_rate: 7.4466e-07\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 6.737951707691536e-07.\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23s/step - loss: 0.0200 - r_squared: 0.0612 - val_loss: 0.0373 - val_r_squared: -0.0751 - learning_rate: 6.7380e-07\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 6.096751121731359e-07.\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 23s/step - loss: 0.0193 - r_squared: 0.0930 - val_loss: 0.0373 - val_r_squared: -0.0750 - learning_rate: 6.0968e-07\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 5.516568535313127e-07.\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 22s/step - loss: 0.0194 - r_squared: 0.0903 - val_loss: 0.0373 - val_r_squared: -0.0748 - learning_rate: 5.5166e-07\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 4.991597961634398e-07.\n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 23s/step - loss: 0.0218 - r_squared: -0.0527 - val_loss: 0.0372 - val_r_squared: -0.0746 - learning_rate: 4.9916e-07\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 4.516584795055678e-07.\n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23s/step - loss: 0.0200 - r_squared: 0.0748 - val_loss: 0.0372 - val_r_squared: -0.0746 - learning_rate: 4.5166e-07\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 4.086774936240545e-07.\n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23s/step - loss: 0.0184 - r_squared: 0.0551 - val_loss: 0.0372 - val_r_squared: -0.0745 - learning_rate: 4.0868e-07\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 3.6978670436838e-07.\n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23s/step - loss: 0.0195 - r_squared: 0.0749 - val_loss: 0.0372 - val_r_squared: -0.0745 - learning_rate: 3.6979e-07\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 3.345968480061856e-07.\n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23s/step - loss: 0.0199 - r_squared: 0.0695 - val_loss: 0.0372 - val_r_squared: -0.0744 - learning_rate: 3.3460e-07\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 3.0275575113591913e-07.\n",
      "Epoch 68/100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "\n",
    "# Extract features and targets\n",
    "X = combined_feature #feature\n",
    "y = VB_data #target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = K.sum(K.square(y_true - y_pred))\n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - ss_res/(ss_tot + K.epsilon()))\n",
    "    \n",
    "# 初始化模型\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='tanh', input_shape=(9000, 3)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001, decay=1e-6,clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='mse',metrics=[r_squared])\n",
    "\n",
    "#学习率衰减\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))\n",
    "lr_scheduler = LearningRateScheduler(scheduler,verbose = 1)\n",
    "\n",
    "#早停\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1,callbacks=[lr_scheduler])\n",
    "\n",
    "# 模型评估\n",
    "loss= model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2053367c-954b-4e4c-91f9-ad1ee5082f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 绘制训练和验证的损失曲线\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制训练和验证的损失曲线\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "plt.plot(history.history['r_squared'], label='Train R^2')\n",
    "plt.plot(history.history['val_r_squared'], label='Validation R^2')\n",
    "plt.title('R^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12a2602-5a20-450b-b659-a75bb489fc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step\n",
      "[0.3205801] [0.24]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "test_data = loadmat('row_110_data.mat')\n",
    "ae_spindle_data = test_data['AE_spindle']\n",
    "vib_spindle_data = test_data['vib_spindle']\n",
    "ae_table_data = test_data['AE_table']\n",
    "vib_table_data = test_data['vib_table']\n",
    "VB_data = test_data['VB']\n",
    "\n",
    "\n",
    "\n",
    "# ae_spindle_reduced = ae_spindle_data[:, start_index_ae:end_index_ae]\n",
    "# vib_spindle_reduced = vib_spindle_data[:, start_index_vb:end_index_vb]\n",
    "# ae_table_reduced = ae_table_data[:, start_index_ae:end_index_ae]\n",
    "# vib_table_reduced = vib_table_data[:, start_index_ae:end_index_ae]\n",
    "\n",
    "\n",
    "combined_feature = np.stack((ae_spindle_data,vib_spindle_data,vib_table_data), axis=-1)\n",
    "\n",
    "\n",
    "# 使用模型进行预测\n",
    "predictions = model.predict(combined_feature)\n",
    "\n",
    "# 选择一个样本来展示预测和实际值\n",
    "sample_index = 0\n",
    "predicted_values = predictions[sample_index]\n",
    "actual_values = VB_data[sample_index]\n",
    "print(predicted_values, actual_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435fa72-1ae5-4fd1-98cd-8300757bc279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
